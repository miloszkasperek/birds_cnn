# -*- coding: utf-8 -*-
"""birds_report_2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1dT7aWVOhkZZ6MoAlR1otajpVn9Qa_9Lh

# Importowanie odpowiednich bibliotek
"""

import keras as k
from keras.preprocessing.image import ImageDataGenerator
from keras.models import Sequential
from keras.layers import Dense, Flatten, Conv2D, MaxPool2D, Dropout, BatchNormalization
from keras.callbacks import EarlyStopping
import matplotlib.pyplot as plt
import pandas as pd
import cv2
import os
import numpy as np
import random
import matplotlib.image as mpimg

"""# Pobieranie zbioru danych

###### przepis na to jak pobrać zbiór z kaggle do google colab wziąłem z: https://www.kaggle.com/general/74235

###### zbiór danych to 41218 zdjęć 270 gatunków ptaków o rozmiarach 224x224 i 3 kanałach koloru

###### podzielony jest na 38518 zdjęć stanowiących zbiór treningowy i po 1350 zdjęć będących zbiorem walidacyjnym i będących zbiorem testowym

###### zbiór dostępny jest na platformie kaggle: https://www.kaggle.com/gpiosenka/100-bird-species
"""

!pip install -q kaggle

from google.colab import files

files.upload()

!mkdir ~/.kaggle

!cp kaggle.json ~/.kaggle/

!chmod 600 ~/.kaggle/kaggle.json

!kaggle datasets list

!kaggle datasets download -d gpiosenka/100-bird-species

!mkdir birds

!unzip 100-bird-species.zip -d birds

"""# Przypisanie ścieżek do podzbiorów do zmiennych"""

train_path = r'/content/birds/train'
valid_path = r'/content/birds/valid'
test_path = r'/content/birds/test'

"""# Pokazanie przykładowych egzemplarzy z każdego gatunku ptaka"""

example_generator = k.preprocessing.image.ImageDataGenerator()
example = example_generator.flow_from_directory('/content/birds/train')
example_labels = list(example.class_indices.keys())
for X, y in example:
    fig, ax = plt.subplots(1, 5, figsize=(20, 20))
    
    for i in range(0,5):
        img = X[i].astype('int')
        label = example_labels[np.argmax(y[i])]
        ax[i].imshow(img)
        ax[i].set_title(label)
        ax[i].set_xticks([])
        ax[i].set_yticks([])

    
    plt.show()

"""# Utworzenie generatora odpowiednich dla sieci neuronowych zbiorów danych"""

generator = ImageDataGenerator(rescale = 1./255)

"""# Utworzenie odpowiednich inputów dla sieci neuronowych

##### rozmiar wsadu ustaliłem na 64 lub 128; w późniejszych etapach projektu okazało się, że 64 daje lepsze wyniki

##### ImageDataGenerator daje możliwość zastosowania one hot encodingu w momencie określania argumentu "class_mode" funkcji flow-from_directory i taką też metodę zastosowałem
"""

train_ds1 = generator.flow_from_directory(
    directory = train_path,
    target_size=(244, 244),
    batch_size = 64,
    color_mode="rgb",
    class_mode="categorical")

valid_ds = generator.flow_from_directory(
    directory = valid_path,
    target_size=(244, 244),
    batch_size = 64,
    color_mode="rgb",
    class_mode="categorical")

test_ds = generator.flow_from_directory(
    directory = test_path,
    target_size=(244, 244),
    batch_size = 64,
    color_mode="rgb",
    class_mode="categorical")


train_ds2 = generator.flow_from_directory(
    directory = train_path,
    target_size=(244, 244),
    batch_size = 128,
    color_mode="rgb",
    class_mode="categorical")

"""# Pierwszy model"""

model1 = k.models.Sequential()
model1.add(k.layers.Conv2D(filters=32, kernel_size=3, activation='relu', input_shape=[244, 244, 3]))
model1.add(k.layers.MaxPool2D(pool_size=2))

model1.add(k.layers.Conv2D(filters=32, kernel_size=3, activation='relu'))
model1.add(k.layers.MaxPool2D(pool_size=2))

model1.add(k.layers.Dropout(0.5))

model1.add(k.layers.Conv2D(filters=64, kernel_size=3, activation='relu'))
model1.add(k.layers.MaxPool2D(pool_size=2))

model1.add(k.layers.Dropout(0.5))

model1.add(k.layers.Conv2D(filters=64, kernel_size=3, activation='relu'))
model1.add(k.layers.MaxPool2D(pool_size=2))

model1.add(k.layers.Dropout(0.5))

model1.add(k.layers.Conv2D(filters=128, kernel_size=3, activation='relu'))
model1.add(k.layers.MaxPool2D(pool_size=2))

model1.add(k.layers.Dropout(0.5))

model1.add(k.layers.Flatten())
model1.add(k.layers.Dense(265, activation = 'relu'))
model1.add(k.layers.Dropout(0.5))

model1.add(k.layers.Dense(265, activation='softmax'))

model1.compile(optimizer = k.optimizers.Adam(0.001), loss = 'categorical_crossentropy', metrics = ['accuracy'])

model1_trained=model1.fit(x=train_ds1, validation_data = valid_ds, epochs = 30)

model1.save(r'./birds/model1_a30.hdf5')

model1.evaluate(test_ds, batch_size=64)

plt.plot(model1_trained.history['loss'])

plt.plot(model1_trained.history['val_loss'])

plt.show()

model1_trained=model1.fit(x=train_ds1, validation_data = valid_ds, epochs = 30)

model1.save(r'./birds/model1_a60.hdf5')

model1.evaluate(test_ds, batch_size=64)

plt.plot(model1_trained.history['loss'])

plt.plot(model1_trained.history['val_loss'])

plt.show()

model1_trained=model1.fit(x=train_ds1, validation_data = valid_ds, epochs = 30)

model1.save(r'./birds/model1_a90.hdf5')

model1.evaluate(test_ds, batch_size=64)

plt.plot(model1_trained.history['loss'])

plt.plot(model1_trained.history['val_loss'])

plt.show()

"""### Model 1 po 90 epokach zatrzymał się na w okolicach 30% accuracy na zbiorach walidacyjnym i testowym, więc zakończyłem uczenie go

# Model 2

### Zmniejszyłem learning rate do 0.0001
"""

model2 = k.models.Sequential()
model2.add(k.layers.Conv2D(filters=32, kernel_size=3, activation='relu', input_shape=[244, 244, 3]))
model2.add(k.layers.MaxPool2D(pool_size=2))

model2.add(k.layers.Conv2D(filters=32, kernel_size=3, activation='relu'))
model2.add(k.layers.MaxPool2D(pool_size=2))

model2.add(k.layers.Dropout(0.5))

model2.add(k.layers.Conv2D(filters=64, kernel_size=3, activation='relu'))
model2.add(k.layers.MaxPool2D(pool_size=2))

model2.add(k.layers.Dropout(0.5))

model2.add(k.layers.Conv2D(filters=64, kernel_size=3, activation='relu'))
model2.add(k.layers.MaxPool2D(pool_size=2))

model2.add(k.layers.Dropout(0.5))

model2.add(k.layers.Conv2D(filters=128, kernel_size=3, activation='relu'))
model2.add(k.layers.MaxPool2D(pool_size=2))

model2.add(k.layers.Dropout(0.5))

model2.add(k.layers.Flatten())
model2.add(k.layers.Dense(265, activation = 'relu'))
model2.add(k.layers.Dropout(0.5))

model2.add(k.layers.Dense(265, activation='softmax'))

model2.compile(optimizer = k.optimizers.Adam(0.0001), loss = 'categorical_crossentropy', metrics = ['accuracy'])

model2_trained=model2.fit(x=train_ds1, validation_data = valid_ds, epochs = 50)

model2.save(r'./birds/model2_a50.hdf5')

model2.evaluate(test_ds, batch_size=64)

plt.plot(model2_trained.history['loss'])

plt.plot(model2_trained.history['val_loss'])

plt.show()

model2_trained=model2.fit(x=train_ds1, validation_data = valid_ds, epochs = 50)

model2.save(r'./birds/model2_a100.hdf5')

model2.evaluate(test_ds, batch_size=64)

plt.plot(model2_trained.history['loss'])

plt.plot(model2_trained.history['val_loss'])

plt.show()

model2_trained=model2.fit(x=train_ds1, validation_data = valid_ds, epochs = 50)

model2.save(r'./birds/model2_a150.hdf5')

model2.evaluate(test_ds, batch_size=64)

plt.plot(model2_trained.history['loss'])

plt.plot(model2_trained.history['val_loss'])

plt.show()

"""### uczenie po 150 epokach zdecydowanie spowolniło, więc przerwałem dalsze uczenie w celu dalszej modyfikacji modelu

# Model 3

### przy tworzeniu modelu nr 3 postanowiłem wykorzystać tzw. "data augmentation" czyli wzbogacenie zbioru treningowego o obrazki powstałe przez obrócenie/przybliżenie/przesunięcie obecnych już obrazków
"""

generator = k.preprocessing.image.ImageDataGenerator(rescale = 1/255,
                                   horizontal_flip=True,
                                   vertical_flip=True,
                                   zoom_range=0.2,
                                   shear_range=0.2,
                                   width_shift_range=0.1,
                                   height_shift_range=0.1,
                                   rotation_range=30,
                                   fill_mode='nearest')

train_ds1 = generator.flow_from_directory(
    directory = train_path,
    target_size=(244, 244),
    batch_size = 64,
    color_mode="rgb",
    class_mode="categorical")

valid_ds = generator.flow_from_directory(
    directory = valid_path,
    target_size=(244, 244),
    batch_size = 64,
    color_mode="rgb",
    class_mode="categorical")

test_ds = generator.flow_from_directory(
    directory = test_path,
    target_size=(244, 244),
    batch_size = 64,
    color_mode="rgb",
    class_mode="categorical")


train_ds2 = generator.flow_from_directory(
    directory = train_path,
    target_size=(244, 244),
    batch_size = 128,
    color_mode="rgb",
    class_mode="categorical")

"""### w samym modelu zmieniłem tylko liczbę klasyfikowanych gatunków, gdyż pojawiła się aktualizacja zbioru danych"""

model3 = k.models.Sequential()
model3.add(k.layers.Conv2D(filters=32, kernel_size=3, activation='relu', input_shape=[244, 244, 3]))
model3.add(k.layers.MaxPool2D(pool_size=2))

model3.add(k.layers.Conv2D(filters=32, kernel_size=3, activation='relu'))
model3.add(k.layers.MaxPool2D(pool_size=2))

model3.add(k.layers.Dropout(0.5))

model3.add(k.layers.Conv2D(filters=64, kernel_size=3, activation='relu'))
model3.add(k.layers.MaxPool2D(pool_size=2))

model3.add(k.layers.Dropout(0.5))

model3.add(k.layers.Conv2D(filters=64, kernel_size=3, activation='relu'))
model3.add(k.layers.MaxPool2D(pool_size=2))

model3.add(k.layers.Dropout(0.5))

model3.add(k.layers.Conv2D(filters=128, kernel_size=3, activation='relu'))
model3.add(k.layers.MaxPool2D(pool_size=2))

model3.add(k.layers.Dropout(0.5))

model3.add(k.layers.Flatten())
model3.add(k.layers.Dense(270, activation = 'relu'))
model3.add(k.layers.Dropout(0.5))

model3.add(k.layers.Dense(270, activation='softmax'))

model3.compile(optimizer = k.optimizers.Adam(0.0001), loss = 'categorical_crossentropy', metrics = ['accuracy'])

model3_trained=model3.fit(x=train_ds1, validation_data = valid_ds, epochs = 25)

model3.save(r'./birds/model3_a25.hdf5')

model3.evaluate(test_ds, batch_size=64)

plt.plot(model3_trained.history['loss'])

plt.plot(model3_trained.history['val_loss'])

plt.show()

"""### po 25 epoce postanowiłem zakończyć uczenie sieci, mimo  dobrych rezultatów, ponieważ trwało ono zbyt długo a tak długie uczenie sieci na google colab jest problematyczne

# Model 4

### wróciłem do uczenia na zwykłym zbiorze, bez data augumentation

### zamieniłem warstwy Dropout na warstwy BatchNormalization

### zostawiłem jedynie jedną warstwę Dropout po pierwszej warstwie gęsto połączonej i ustaliłem jej argument na 0.3
"""

model4 = k.models.Sequential()
model4.add(k.layers.Conv2D(filters=32, kernel_size=3, activation='relu', input_shape=[244, 244, 3]))
model4.add(k.layers.MaxPool2D(pool_size=2))

model4.add(k.layers.Conv2D(filters=32, kernel_size=3, activation='relu'))
model4.add(k.layers.MaxPool2D(pool_size=2))

model4.add(k.layers.BatchNormalization())

model4.add(k.layers.Conv2D(filters=64, kernel_size=3, activation='relu'))
model4.add(k.layers.MaxPool2D(pool_size=2))

model4.add(k.layers.BatchNormalization())

model4.add(k.layers.Conv2D(filters=64, kernel_size=3, activation='relu'))
model4.add(k.layers.MaxPool2D(pool_size=2))

model4.add(k.layers.BatchNormalization())

model4.add(k.layers.Conv2D(filters=128, kernel_size=3, activation='relu'))
model4.add(k.layers.MaxPool2D(pool_size=2))

model4.add(k.layers.BatchNormalization())

model4.add(k.layers.Flatten())
model4.add(k.layers.Dense(270, activation = 'relu'))
model4.add(k.layers.Dropout(0.3))

model4.add(k.layers.Dense(270, activation='softmax'))

model4.compile(optimizer = k.optimizers.Adam(0.0001), loss = 'categorical_crossentropy', metrics = ['accuracy'])

model4_trained1=model4.fit(x=train_ds1, validation_data = valid_ds, epochs = 50)

model4.save(r'./birds/model4_a50.hdf5')

model4.evaluate(test_ds, batch_size=64)

plt.plot(model4_trained1.history['loss'])

plt.plot(model4_trained1.history['val_loss'])

plt.show()

"""### po 50 epokach zauważyłem, że sieć się przeuczyła i nie daje już lepszych rezultatów, więc zakończyłem uczenie

# Model 5

### w modelu 5 zmieniłem tylko wartość Dropoutu na 0.4, wyniki po 100 epokach nie były lepsze od modelu 4, ale niestety output nie zapisał się i nie mam jak go pokazać

# Model 6

### w modelu 6 postanowiłem ustalić wartość Dropoutu na 0.4 oraz zmienić kernel size w warstwach konwolucyjnych 1 i 2 na 7 oraz w warstwach 3 i 4 na 5; rozwiązanie to zauważyłem tutaj: https://miroslawmamczur.pl/sieci-konwolucyjne-do-rozpoznawania-zapalenia-pluc/
"""

model6 = k.models.Sequential()
model6.add(k.layers.Conv2D(filters=32, kernel_size=7, activation='relu', input_shape=[244, 244, 3]))
model6.add(k.layers.MaxPool2D(pool_size=2))

model6.add(k.layers.Conv2D(filters=32, kernel_size=7, activation='relu'))
model6.add(k.layers.MaxPool2D(pool_size=2))

model6.add(k.layers.BatchNormalization())

model6.add(k.layers.Conv2D(filters=64, kernel_size=5, activation='relu'))
model6.add(k.layers.MaxPool2D(pool_size=2))

model6.add(k.layers.BatchNormalization())

model6.add(k.layers.Conv2D(filters=64, kernel_size=5, activation='relu'))
model6.add(k.layers.MaxPool2D(pool_size=2))

model6.add(k.layers.BatchNormalization())

model6.add(k.layers.Conv2D(filters=128, kernel_size=3, activation='relu'))
model6.add(k.layers.MaxPool2D(pool_size=2))

model6.add(k.layers.BatchNormalization())

model6.add(k.layers.Conv2D(filters=128, kernel_size=3, activation='relu'))
model6.add(k.layers.MaxPool2D(pool_size=2))

model6.add(k.layers.BatchNormalization())

model6.add(k.layers.Flatten())
model6.add(k.layers.Dense(270, activation = 'relu'))
model6.add(k.layers.Dropout(0.4))

model6.add(k.layers.Dense(270, activation='softmax'))

model6.compile(optimizer = k.optimizers.Adam(0.0001), loss = 'categorical_crossentropy', metrics = ['accuracy'])

model6_trained1=model6.fit(x=train_ds1, validation_data = valid_ds, epochs = 50)

model6.save(r'./birds/model6_a50.hdf5')

model6.evaluate(test_ds, batch_size=64)

plt.plot(model6_trained1.history['loss'])

plt.plot(model6_trained1.history['val_loss'])

plt.show()

"""### około epoki 20 sieć zaczęła się przeuczać i nie dawała lepszych rezultatów, więc po 50 epoce zakończyłem uczenie

# Model 7

### model 7 jest podobny do modelu 2, z tą różnicą, że w modelu 7 po warstwie konwolucyjnej nr 5 została dodana jeszcze jedna taka sama warstwa wraz z warstwą Dropout po niej
"""

model7 = k.models.Sequential()
model7.add(k.layers.Conv2D(filters=32, kernel_size=3, activation='relu', input_shape=[244, 244, 3]))
model7.add(k.layers.MaxPool2D(pool_size=2))

model7.add(k.layers.Conv2D(filters=32, kernel_size=3, activation='relu'))
model7.add(k.layers.MaxPool2D(pool_size=2))

model7.add(k.layers.Dropout(0.5))

model7.add(k.layers.Conv2D(filters=64, kernel_size=3, activation='relu'))
model7.add(k.layers.MaxPool2D(pool_size=2))

model7.add(k.layers.Dropout(0.5))

model7.add(k.layers.Conv2D(filters=64, kernel_size=3, activation='relu'))
model7.add(k.layers.MaxPool2D(pool_size=2))

model7.add(k.layers.Dropout(0.5))

model7.add(k.layers.Conv2D(filters=128, kernel_size=3, activation='relu'))
model7.add(k.layers.MaxPool2D(pool_size=2))

model7.add(k.layers.Dropout(0.5))

model7.add(k.layers.Conv2D(filters=128, kernel_size=3, activation='relu'))
model7.add(k.layers.MaxPool2D(pool_size=2))

model7.add(k.layers.Dropout(0.5))
model7.add(k.layers.Flatten())
model7.add(k.layers.Dense(270, activation = 'relu'))
model7.add(k.layers.Dropout(0.5))

model7.add(k.layers.Dense(270, activation='softmax'))

model7.compile(optimizer = k.optimizers.Adam(0.0001), loss = 'categorical_crossentropy', metrics = ['accuracy'])

model7_trained1=model7.fit(x=train_ds1, validation_data = valid_ds, epochs = 50)

model7.save(r'./birds/model7_a50.hdf5')

model7.evaluate(test_ds, batch_size=64)

plt.plot(model7_trained1.history['loss'])

plt.plot(model7_trained1.history['val_loss'])

plt.show()

model7_trained1=model7.fit(x=train_ds1, validation_data = valid_ds, epochs = 50)

"""### po niemal 100 epokach wyniki tej sieci nie były zbyt dobre a uczenie sieci spowalnia

### nie widząc nadziei na lepsze rezultaty, zakończyłem więc uczenie tej sieci

# Model 8

### w modelu 8 postanowiłem uprośćić sieć poprzez zostawienie tylko 3 warstw konwolucyjnych o różnej liczbie filtrów wraz z następującymi po nich warstwami Dropout; reszta bez zmian
"""

model8 = k.models.Sequential()
model8.add(k.layers.Conv2D(filters=32, kernel_size=3, activation='relu', input_shape=[244, 244, 3]))
model8.add(k.layers.MaxPool2D(pool_size=2))


model8.add(k.layers.Dropout(0.5))

model8.add(k.layers.Conv2D(filters=64, kernel_size=3, activation='relu'))
model8.add(k.layers.MaxPool2D(pool_size=2))

model8.add(k.layers.Dropout(0.5))


model8.add(k.layers.Conv2D(filters=128, kernel_size=3, activation='relu'))
model8.add(k.layers.MaxPool2D(pool_size=2))

model8.add(k.layers.Dropout(0.5))

model8.add(k.layers.Flatten())
model8.add(k.layers.Dense(270, activation = 'relu'))
model8.add(k.layers.Dropout(0.5))

model8.add(k.layers.Dense(270, activation='softmax'))

model8.compile(optimizer = k.optimizers.Adam(0.0001), loss = 'categorical_crossentropy', metrics = ['accuracy'])

model8_trained1=model8.fit(x=train_ds1, validation_data = valid_ds, epochs = 50)

"""### po 4 epokach sieć nic się nie nauczyła, więc przerwałem uczenie

# Model 9

### w tym modelu zastosowałem jednocześnie warstwy Dropout jak i warstwy BatchNormalization po warstwach konwolucyjnych
"""

model9 = k.models.Sequential()
model9.add(k.layers.Conv2D(filters=32, kernel_size=3, activation='relu', input_shape=[244, 244, 3]))
model9.add(k.layers.MaxPool2D(pool_size=2))
model9.add(k.layers.BatchNormalization())

model9.add(k.layers.Conv2D(filters=32, kernel_size=3, activation='relu'))
model9.add(k.layers.MaxPool2D(pool_size=2))
model9.add(k.layers.BatchNormalization())

model9.add(k.layers.Dropout(0.5))

model9.add(k.layers.Conv2D(filters=64, kernel_size=3, activation='relu'))
model9.add(k.layers.MaxPool2D(pool_size=2))
model9.add(k.layers.BatchNormalization())

model9.add(k.layers.Dropout(0.5))

model9.add(k.layers.Conv2D(filters=64, kernel_size=3, activation='relu'))
model9.add(k.layers.MaxPool2D(pool_size=2))
model9.add(k.layers.BatchNormalization())

model9.add(k.layers.Dropout(0.5))

model9.add(k.layers.Conv2D(filters=128, kernel_size=3, activation='relu'))
model9.add(k.layers.MaxPool2D(pool_size=2))
model9.add(k.layers.BatchNormalization())

model9.add(k.layers.Dropout(0.5))

model9.add(k.layers.Flatten())
model9.add(k.layers.Dense(270, activation = 'relu'))
model9.add(k.layers.Dropout(0.5))

model9.add(k.layers.Dense(270, activation='softmax'))

model9.compile(optimizer = k.optimizers.Adam(0.0001), loss = 'categorical_crossentropy', metrics = ['accuracy'])

model9_trained1=model9.fit(x=train_ds1, validation_data = valid_ds, epochs = 50)

model9.save(r'./birds/model9_a50.hdf5')

model9.evaluate(test_ds, batch_size=64)

plt.plot(model9_trained1.history['loss'])

plt.plot(model9_trained1.history['val_loss'])

plt.show()

model9_trained1=model9.fit(x=train_ds1, validation_data = valid_ds, epochs = 50)

model9.save(r'./birds/model9_a80.hdf5')

model9.evaluate(test_ds, batch_size=64)

plt.plot(model9_trained1.history['loss'])

plt.plot(model9_trained1.history['val_loss'])

plt.show()

"""### po 80 epokach sieć nie wyglądała jakby miała osiągnąć lepsze rezultaty niż model 2, więc zakończyłem uczenie

# Model 10

### model 10 jest taki sam jak model 2

### postanowiłem jednak jeszcze raz go przeliczyć, aby zobaczyć czy zmiana z 265 na 270 rozpoznawanych gatunków nie wpłynie na wyniki
"""

model10 = k.models.Sequential()
model10.add(k.layers.Conv2D(filters=32, kernel_size=3, activation='relu', input_shape=[244, 244, 3]))
model10.add(k.layers.MaxPool2D(pool_size=2))

model10.add(k.layers.Conv2D(filters=32, kernel_size=3, activation='relu'))
model10.add(k.layers.MaxPool2D(pool_size=2))

model10.add(k.layers.Dropout(0.5))

model10.add(k.layers.Conv2D(filters=64, kernel_size=3, activation='relu'))
model10.add(k.layers.MaxPool2D(pool_size=2))

model10.add(k.layers.Dropout(0.5))

model10.add(k.layers.Conv2D(filters=64, kernel_size=3, activation='relu'))
model10.add(k.layers.MaxPool2D(pool_size=2))

model10.add(k.layers.Dropout(0.5))

model10.add(k.layers.Conv2D(filters=128, kernel_size=3, activation='relu'))
model10.add(k.layers.MaxPool2D(pool_size=2))

model10.add(k.layers.Dropout(0.5))

model10.add(k.layers.Flatten())
model10.add(k.layers.Dense(270, activation = 'relu'))
model10.add(k.layers.Dropout(0.5))

model10.add(k.layers.Dense(270, activation='softmax'))

model10.compile(optimizer = k.optimizers.Adam(0.0001), loss = 'categorical_crossentropy', metrics = ['accuracy'])

model10_trained1=model10.fit(x=train_ds1, validation_data = valid_ds, epochs = 50)

model10.save(r'./birds/model10_a50.hdf5')

model10.evaluate(test_ds, batch_size=64)

plt.plot(model10_trained1.history['loss'])

plt.plot(model10_trained1.history['val_loss'])

plt.show()

plt.plot(model10_trained1.history['accuracy'])

plt.plot(model10_trained1.history['val_accuracy'])

"""### jak się okazało, dodanie 5 nowych gatunków nie wpłynęło znacząco na wyniki

# Model 11

### różni się od modelu nr 2 współczynnikiem uczenia (z 0.0001 na 0.0005)
"""

model11 = k.models.Sequential()
model11.add(k.layers.Conv2D(filters=32, kernel_size=3, activation='relu', input_shape=[244, 244, 3]))
model11.add(k.layers.MaxPool2D(pool_size=2))

model11.add(k.layers.Conv2D(filters=32, kernel_size=3, activation='relu'))
model11.add(k.layers.MaxPool2D(pool_size=2))

model11.add(k.layers.Dropout(0.5))

model11.add(k.layers.Conv2D(filters=64, kernel_size=3, activation='relu'))
model11.add(k.layers.MaxPool2D(pool_size=2))

model11.add(k.layers.Dropout(0.5))

model11.add(k.layers.Conv2D(filters=64, kernel_size=3, activation='relu'))
model11.add(k.layers.MaxPool2D(pool_size=2))

model11.add(k.layers.Dropout(0.5))

model11.add(k.layers.Conv2D(filters=128, kernel_size=3, activation='relu'))
model11.add(k.layers.MaxPool2D(pool_size=2))

model11.add(k.layers.Dropout(0.5))

model11.add(k.layers.Flatten())
model11.add(k.layers.Dense(270, activation = 'relu'))
model11.add(k.layers.Dropout(0.5))

model11.add(k.layers.Dense(270, activation='softmax'))

model11.compile(optimizer = k.optimizers.Adam(0.0005), loss = 'categorical_crossentropy', metrics = ['accuracy'])

model11_trained1=model11.fit(x=train_ds1, validation_data = valid_ds, epochs = 50)

model11.save(r'./birds/model11_a50.hdf5')

model11.evaluate(test_ds, batch_size=64)

plt.plot(model11_trained1.history['loss'])

plt.plot(model11_trained1.history['val_loss'])

plt.show()

plt.plot(model11_trained1.history['accuracy'])

plt.plot(model11_trained1.history['val_accuracy'])

plt.show()

"""### wyniki nie są lepsze niż w przypadku modelu nr 2

# Model 12

### różni się od modelu nr 2 współczynnikiem uczenia (z 0.0001 na 0.00001)
"""

model12 = k.models.Sequential()
model12.add(k.layers.Conv2D(filters=32, kernel_size=3, activation='relu', input_shape=[244, 244, 3]))
model12.add(k.layers.MaxPool2D(pool_size=2))

model12.add(k.layers.Conv2D(filters=32, kernel_size=3, activation='relu'))
model12.add(k.layers.MaxPool2D(pool_size=2))

model12.add(k.layers.Dropout(0.5))

model12.add(k.layers.Conv2D(filters=64, kernel_size=3, activation='relu'))
model12.add(k.layers.MaxPool2D(pool_size=2))

model12.add(k.layers.Dropout(0.5))

model12.add(k.layers.Conv2D(filters=64, kernel_size=3, activation='relu'))
model12.add(k.layers.MaxPool2D(pool_size=2))

model12.add(k.layers.Dropout(0.5))

model12.add(k.layers.Conv2D(filters=128, kernel_size=3, activation='relu'))
model12.add(k.layers.MaxPool2D(pool_size=2))

model12.add(k.layers.Dropout(0.5))

model12.add(k.layers.Flatten())
model12.add(k.layers.Dense(270, activation = 'relu'))
model12.add(k.layers.Dropout(0.5))

model12.add(k.layers.Dense(270, activation='softmax'))

model12.compile(optimizer = k.optimizers.Adam(0.00001), loss = 'categorical_crossentropy', metrics = ['accuracy'])

model12_trained1=model12.fit(x=train_ds1, validation_data = valid_ds, epochs = 50)

model12.save(r'./birds/model12_a50.hdf5')

model12.evaluate(test_ds, batch_size=64)

plt.plot(model12_trained1.history['loss'])

plt.plot(model12_trained1.history['val_loss'])

plt.show()

"""### po 50 epokach sieć uzyskała 7,3% accuracy na zbiorze testowym, więc jest to o wiele wolniejsze tempo uczenia niż w przypadku modelu nr 2

### na colabie uczenie tej sieci byłoby problematyczne i czasochłonne, więc zakończyłem uczenie tej sieci

# Podsumowanie

W powyższym projekcie widocznych jest 12 prób wytrenowania sieci. Było ich więcej, jednak ze względu na to, że były bardzo eksperymentalne i/lub mało wnosiły i/lub ich rezultaty były bardzo niezadowalające i/lub ich wyniki się wcale nie zapisały, postanowiłem umieścić tylko 12 z nich.

Najlepszy rezultat uzyskała sieć nr 2 (80% na zbiorze testowym). Prawdopodobnie, że gdyby epok było więcej, sieć uzyskała by jeszcze lepszy rezultat. Jeśli zaś chodzi o pozostałe modele, to na pewno modele 3, 7 i 12 mogłyby być dłużej uczone i możliwe, że uzyskałyby wyniki nawet lepsze od modelu nr 2. Szczególnie warty do sprawdzenia w przyszłości byłby model nr 3, gdyż zastosowana w nim technika "data augmentation" powinna w teorii poprawić uczenie się sieci.

Dużym problemem przy próbie zrobienia tego projektu okazały się problemy czysto techniczne. Google Colab jest dobrym narzędziem, ale ma swoje ograniczenia przez co trenowanie sieci wymagało o wiele więcej czasu. Przeliczenie sieci na zewnętrznym serwerze mogłoby być rozwiązaniem, jednak nie daje ono możliwości modyfikowania sprawdzania wyników sieci na bieżąco oraz trwa dłużej niż na Colabie.

Podsumowując, projekt na pewno wymaga dopracowania a jego wyniki nie są rewelacyjne, jednak daje to na pewno motywację do dalszej nauki i dalszych prób.
"""



